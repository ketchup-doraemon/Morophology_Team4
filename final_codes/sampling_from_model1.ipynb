{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Daisuke Yoda'\n",
    "__Date__ = 'December 2018'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import os\n",
    "__dir__ = os.getcwd()[:-11]\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(__dir__ + 'data/glove.6B.100d.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(words, max_len =6):\n",
    "    patterns = defaultdict(list)\n",
    "\n",
    "    for word in words:\n",
    "        for second_word in words:\n",
    "            if word != second_word:\n",
    "                i = 1\n",
    "                while(word[:i]==second_word[:i]):\n",
    "                    i += 1\n",
    "                if i != 1 and i > max(len(word[i-1:]), len(second_word[i-1:])) < max_len:\n",
    "                    if (\"suffix\", word[i-1:], second_word[i-1:]) in patterns:\n",
    "                        patterns[(\"suffix\", word[i-1:], second_word[i-1:])].append((word, second_word))\n",
    "                    else:\n",
    "                        patterns[(\"suffix\", word[i-1:], second_word[i-1:])] = [(word, second_word)]\n",
    "                        \n",
    "                i = 1\n",
    "                while(word[-i:]==second_word[-i:]):\n",
    "                    i += 1\n",
    "                if i != 1 and max(len(word[:-i+1]), len(second_word[:-i+1])) < max_len:\n",
    "                    if (\"prefix\", word[:-i+1], second_word[:-i+1]) in patterns:\n",
    "                        patterns[(\"prefix\", word[:-i+1], second_word[:-i+1])].append((word, second_word))\n",
    "                    else:\n",
    "                        patterns[(\"prefix\", word[:-i+1], second_word[:-i+1])] = [(word, second_word)]\n",
    "                        \n",
    "    return patterns\n",
    "\n",
    "\n",
    "def molph_classify(thepairs,model,threshold=0.5,min_category=5):\n",
    "    new_pairs = defaultdict(list)\n",
    "\n",
    "    for key in thepairs:\n",
    "        cadidates = thepairs[key]\n",
    "        \n",
    "        similality = []\n",
    "        for pair in cadidates:\n",
    "            try:\n",
    "                cos_sim = model(pair[0],pair[1])\n",
    "            except:\n",
    "                pass\n",
    "            else: \n",
    "                if cos_sim > threshold:\n",
    "                    similality.append(pair + (cos_sim,))\n",
    "        \n",
    "        if len(similality) > min_category :\n",
    "             new_pairs[key] = similality\n",
    "             \n",
    "    return new_pairs\n",
    "\n",
    "\n",
    "def make_same_group(pairs,word):\n",
    "    pair_list = sum(list(pairs.values()),[])\n",
    "    group = [pair for pair in pair_list if word==(pair[0] or pair[1])]\n",
    "            \n",
    "    return group\n",
    "\n",
    "\n",
    "def plot_graph(pair_group):\n",
    "    G = nx.Graph()\n",
    "    for pair in pair_group:\n",
    "        G.add_nodes_from([pair[0],pair[1]])\n",
    "        G.add_edge(pair[0],pair[1])\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    pos = nx.spring_layout(G,k=0.7)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightgray', node_size=5000.0)\n",
    "    nx.draw_networkx_edges(G, pos, width=2.5)\n",
    "    nx.draw_networkx_labels(G, pos, fontsize=25, font_weight=\"bold\")\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_hit_rate(pairs, words):\n",
    "    hit_word = []\n",
    "    pair_list = sum(list(pairs.values ()), [])\n",
    "    for word in words:\n",
    "        hit_word.extend([pair[1] for pair in pair_list if word == pair[0]])\n",
    "        hit_word.extend([pair[0] for pair in pair_list if word == pair[1]])\n",
    "\n",
    "    c = Counter(hit_word)\n",
    "    \n",
    "    return [(pair[0],int(pair[1]/2)) for pair in c.most_common(100)]\n",
    "    \n",
    "\n",
    "def clustering_group(pairs):\n",
    "    pair_list1 = [set(pair[0:2]) for pair in sum(list(pairs.values()), [])]\n",
    "    pair_list2 = deepcopy(pair_list1)\n",
    "    cluster_list = []\n",
    "    for pair1 in pair_list1[:]:\n",
    "        ald = False\n",
    "        for cluster in cluster_list:\n",
    "            if pair1 <= cluster:\n",
    "                ald = True\n",
    "        if ald:\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            calc = False\n",
    "            for pair2 in pair_list2[:]:\n",
    "                if pair1 >= pair2:\n",
    "                    pass\n",
    "                elif not pair1.isdisjoint (pair2):\n",
    "                    pair1 = pair1 | pair2\n",
    "                    calc = True\n",
    "            if not calc:\n",
    "                cluster_list.append (pair1)\n",
    "                break\n",
    "\n",
    "    return cluster_list\n",
    "\n",
    "def get_stem(cluster):\n",
    "    stem_candidate = []\n",
    "    for pair in combinations(cluster,2):\n",
    "        stem_candidate.append([char1 for char1,char2 in zip(pair[0],pair[1]) if char1 == char2])\n",
    "\n",
    "    stem = sorted(stem_candidate,key=lambda x: len(x))\n",
    "\n",
    "    return ''.join(stem[0])\n",
    "\n",
    "\n",
    "def vector_by_stem(stem_cluster,pd_frame=True):\n",
    "    split_vec = {}\n",
    "    for stem in stem_cluster.keys():\n",
    "        for word in stem_cluster[stem]:\n",
    "            if word[:len(stem)] == stem:\n",
    "                vec = np.eye(len(word))[len(stem) - 1]\n",
    "\n",
    "\n",
    "            elif word[::-1][:len(stem[::-1])] == stem[::-1]:\n",
    "                vec = np.eye(len(word))[len(stem)][::-1]\n",
    "\n",
    "\n",
    "            if pd_frame:\n",
    "                split_vec[word] = pd.Series(vec)\n",
    "            else:\n",
    "                split_vec[word] = vec\n",
    "\n",
    "    return pd.DataFrame(split_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'aaron', 'aau', 'abandoned', 'abandonment']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(__dir__ + 'data/english_brown.txt') as f:\n",
    "    data = f.read()\n",
    "    data = data.replace('.','')\n",
    "    data = data.replace(',','')\n",
    "    data = data.replace('\"\"','')\n",
    "    data = data.lower()\n",
    "\n",
    "all_words = data.split()\n",
    "words_set = np.unique(all_words)\n",
    "words_set = [word for word in words_set if word.isalpha()]\n",
    "words_set[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for comparing consine similalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word_vectors.similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making cadidates with same suffix and prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pair = make_pairs(words_set, max_len =6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caluculating the cosine similalities to eliminate the unrelated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apply', 'applied', 0.7922291),\n",
       " ('carry', 'carried', 0.7707906),\n",
       " ('deny', 'denied', 0.7732344),\n",
       " ('justify', 'justified', 0.78390497),\n",
       " ('qualify', 'qualified', 0.7768942),\n",
       " ('study', 'studied', 0.70555717),\n",
       " ('testify', 'testified', 0.74732226),\n",
       " ('vary', 'varied', 0.7457148)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = molph_classify(original_pair,model,threshold=0.7,min_category=5)\n",
    "pairs[('suffix', 'y', 'ied')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['age', 'aged', 'ages'],\n",
       " ['agree', 'agreed', 'agreeing', 'agreement', 'agreements'],\n",
       " ['announce', 'announced', 'announcing', 'announcement'],\n",
       " ['appreciate', 'appreciated', 'appreciation'],\n",
       " ['assure', 'assured', 'assuring'],\n",
       " ['believe', 'believed', 'believes', 'believing'],\n",
       " ['birdie', 'birdied', 'birdies'],\n",
       " ['cause', 'causes', 'caused'],\n",
       " ['celebrate', 'celebrates', 'celebrated', 'celebrating', 'celebration'],\n",
       " ['change', 'changed', 'changes', 'changing']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_cluster = clustering_group(pairs)\n",
    "morph_cluster = [pair for pair in morph_cluster if len(pair) > 2]\n",
    "morph_cluster = [sorted(pair,key=lambda x: len(x)) for pair in morph_cluster]\n",
    "morph_cluster[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stem word to generate split point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aged</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ages</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agreed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5   6   7   8   9   10  11  12  13\n",
       "age     0.0  0.0  1.0  NaN  NaN  NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
       "aged    0.0  0.0  1.0  0.0  NaN  NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
       "ages    0.0  0.0  1.0  0.0  NaN  NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
       "agree   0.0  0.0  0.0  0.0  1.0  NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
       "agreed  0.0  0.0  0.0  0.0  1.0  0.0 NaN NaN NaN NaN NaN NaN NaN NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_cluster = {get_stem(cluster):cluster for cluster in morph_cluster}\n",
    "split_list = vector_by_stem(stem_cluster)\n",
    "split_list.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
